{"index":{"version":"0.5.12","fields":[{"name":"title","boost":10},{"name":"keywords","boost":15},{"name":"body","boost":1}],"ref":"url","documentStore":{"store":{"./":["experience.","includ","introduct","iwiki","person","tip","wich","wiki"],"implementation_scheme/":["implement","scheme"],"implementation_scheme/mixed_precision_training_implementation_scheme.html":["implement","mix","precis","scheme","train"],"tech_tools/":["blog","tech","tool"],"tech_tools/git/":["git","tip","tool","us"],"tech_tools/git/gitbook_on_github-page.html":["\"./_book\"","\"gener","\"setting\"","\"the","\"updat","#","./","./*","./_book.","./master_barnch/_book/*","1.","2.","3.","4.","`gh","`git","`gitbook","`master_barnch`","abov","add","alreadi","automat","b","back","boart","branch","branch,","branch.","branch:","build","build`","built","by:","call","checkout","clone","clone`,","code","codes,","codes:","commit","congrates!","current","dependencies,","directli","docs\"","don't","done,","e.g.,","everyth","file","files.","first","follow","gener","gh","git","gitbook","gitbook'","github","github,","github:","go","init","instal","instruction,","is,","iwiki.","localhost.","locat","m","master","master_barnch","method","move","mv","name.","need","need,","notic","now","origin","page","pages.","pages`","path","path,","push","r","readme.md","replac","repres","respositori","respository\"","respository'","rf","rm","root","run","save","see","serv","serve`","sourc","state","static","summary.md","then,","time","updates\"","url","us","visit","way,","websit","you."],"paper_reading/":["paper","read"],"paper_reading/model_compression/":["compress","model"],"paper_reading/model_compression/quantization_summary.html":["\"averaging\"","\"averaging\",","\"compensate\".","\"dynam","\"dynamic\"","1.","1.1.","1.1.1.","1.2.","1.2.1.","1.2.2.","3.","activ","averag","back","batch","calcul","clip","control","data","deep","differ","dure","dynam","effect","execut","exponent.","factor","fix","flow","function,","gradient","hand,","higher","idea","includ","key","kind","loss","low","lower","mean","multipl","network","neural","oper","operation,","operations.","pact","page","paper","parameter","point\",","precis","precision,","propogation,","quantiz","refer","samples,","scale","share","such","summari","tradit","train","uniqu","us"],"paper_reading/model_compression/quantization/":["quantiz"],"paper_reading/model_compression/quantization/mixed_precision_training.html":["\"autom","\"compressed\"","\"dynam","\"mix","\"train","(default","(henc","0.001).","1.","2)","2,","2.","2.1.","2.1.1.","2.1.2.","2000","absolut","adjust","algorithms,","approach,","author","automat","backoff","begin","both","call","case","check","choos","comput","concretely,","constant","data","decreas","deep","distribut","dure","dynam","each","easi","end","ensur","estim","event","extens","factor","factor\"","factor\".","factor,","factor.","first","fix","float16","float16,","focu","furthermore,","gradient","gradients.","hand,","happen","happens,","here","https://arxiv.org/abs/1710.03740","https://devblogs.nvidia.com/apex","https://github.com/nvidia/apex","https://nvidia.github.io/openseq2seq/html/mix","implement","implementation.","increas","industri","inspect","instance,","inter","introduc","introduct","iter","iteration.","keep","knowledg","larg","last,","less","log","lognorm","loss","low","lower","maximum","mean","means,","mention","mentioned,","method","mix","model","more","multiplications\".","name),","network","neural","next","normal","now","number","on","one.","openseq2seq","optim","overflow","overflow,","paper","paramet","period","point\"","possibl","precis","precision.html#","presenc","probabl","put","pytorch","rang","rare","read","realli","recip","reference:","repres","respectively).","rule","run","s.t.","scale","scale.","scaling.","select","set","shift","similiar","skip","static","statistics,","such","summari","support","system","tech","them.","time,","tip","togeth","train","training\"","training.","training/","two","type,","underflow","underli","understand","understand,","update.","us","valu","varianc","whenev"]},"length":11},"tokenStore":{"root":{"0":{"docs":{},".":{"0":{"0":{"1":{"docs":{},")":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}},"docs":{}},"docs":{}},"docs":{}}},"1":{"docs":{},".":{"1":{"docs":{},".":{"1":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}},"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}},"2":{"docs":{},".":{"1":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}},"2":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}},"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}},"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345},"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}},"2":{"0":{"0":{"0":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}},"docs":{}},"docs":{}},"docs":{},".":{"1":{"docs":{},".":{"1":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}},"2":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}},"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}},"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}},")":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}},"3":{"docs":{},".":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345},"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.022988505747126436}}}},"4":{"docs":{},".":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}},"docs":{},"e":{"docs":{},"x":{"docs":{},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},"i":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.14285714285714285}}}}}}}}}},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}}}}},"e":{"docs":{},"c":{"docs":{},"u":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}},"t":{"docs":{},"e":{"docs":{},"n":{"docs":{},"s":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.01079136690647482}}}}}}},".":{"docs":{},"g":{"docs":{},".":{"docs":{},",":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"y":{"docs":{},"t":{"docs":{},"h":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}},"n":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},"f":{"docs":{},"f":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}}},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}},"s":{"docs":{},"i":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.01079136690647482}}}}},"n":{"docs":{},"d":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}},"s":{"docs":{},"u":{"docs":{},"r":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}},"i":{"docs":{},"n":{"docs":{},"c":{"docs":{},"l":{"docs":{},"u":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.14285714285714285},"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.022988505747126436}}}}},"r":{"docs":{},"e":{"docs":{},"a":{"docs":{},"s":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}}}}},"t":{"docs":{},"r":{"docs":{},"o":{"docs":{},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}},"t":{"docs":{"./":{"ref":"./","tf":10},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}},"e":{"docs":{},"r":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}}},"i":{"docs":{},"t":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"l":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}},"r":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}}}}}},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}},"d":{"docs":{},"u":{"docs":{},"s":{"docs":{},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}},"w":{"docs":{},"i":{"docs":{},"k":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.14285714285714285}},".":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"implementation_scheme/":{"ref":"implementation_scheme/","tf":5.5},"implementation_scheme/mixed_precision_training_implementation_scheme.html":{"ref":"implementation_scheme/mixed_precision_training_implementation_scheme.html","tf":2.2},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}}}}}}}},"s":{"docs":{},",":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}},"d":{"docs":{},"e":{"docs":{},"a":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.017985611510791366}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}}}}}}}}}},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"o":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.14285714285714285}}}}},"i":{"docs":{},"o":{"docs":{},"d":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{"implementation_scheme/mixed_precision_training_implementation_scheme.html":{"ref":"implementation_scheme/mixed_precision_training_implementation_scheme.html","tf":2.2},"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.034482758620689655},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":3.3549160671462825}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}},".":{"docs":{},"h":{"docs":{},"t":{"docs":{},"m":{"docs":{},"l":{"docs":{},"#":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}}}}}},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}},"o":{"docs":{},"p":{"docs":{},"o":{"docs":{},"g":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}}}}}}},"b":{"docs":{},"a":{"docs":{},"b":{"docs":{},"l":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}},"a":{"docs":{},"g":{"docs":{},"e":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":3.3721682847896437},"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.022988505747126436}},"s":{"docs":{},".":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}},"`":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}},"t":{"docs":{},"h":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}},",":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{"paper_reading/":{"ref":"paper_reading/","tf":5.5},"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.014388489208633094}}}}},"c":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}},"r":{"docs":{},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.017985611510791366}},"e":{"docs":{},"r":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}}}}}},"u":{"docs":{},"s":{"docs":{},"h":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.019417475728155338}}}},"t":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}},"o":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"\"":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}},",":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}}},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"b":{"docs":{},"l":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}},"y":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"c":{"docs":{},"h":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}},"t":{"docs":{},"i":{"docs":{},"p":{"docs":{"./":{"ref":"./","tf":0.14285714285714285},"tech_tools/git/":{"ref":"tech_tools/git/","tf":0.3333333333333333},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}},"m":{"docs":{},"e":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"implementation_scheme/mixed_precision_training_implementation_scheme.html":{"ref":"implementation_scheme/mixed_precision_training_implementation_scheme.html","tf":2.2},"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":3.3369304556354913}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"\"":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}},"/":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}},"d":{"docs":{},"i":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}}},"e":{"docs":{},"c":{"docs":{},"h":{"docs":{"tech_tools/":{"ref":"tech_tools/","tf":5.5},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}},"o":{"docs":{},"o":{"docs":{},"l":{"docs":{"tech_tools/":{"ref":"tech_tools/","tf":5},"tech_tools/git/":{"ref":"tech_tools/git/","tf":0.3333333333333333}}}},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{},"h":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}},"h":{"docs":{},"e":{"docs":{},"n":{"docs":{},",":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}},"m":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},"w":{"docs":{},"o":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.01079136690647482}}}},"y":{"docs":{},"p":{"docs":{},"e":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}},"w":{"docs":{},"i":{"docs":{},"c":{"docs":{},"h":{"docs":{"./":{"ref":"./","tf":0.14285714285714285}}}},"k":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.14285714285714285}}}}},"a":{"docs":{},"y":{"docs":{},",":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}},"e":{"docs":{},"b":{"docs":{},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.024271844660194174}}}}}}},"h":{"docs":{},"e":{"docs":{},"n":{"docs":{},"e":{"docs":{},"v":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}},"s":{"docs":{},"c":{"docs":{},"h":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{"implementation_scheme/":{"ref":"implementation_scheme/","tf":5.5},"implementation_scheme/mixed_precision_training_implementation_scheme.html":{"ref":"implementation_scheme/mixed_precision_training_implementation_scheme.html","tf":2.2}}}}}},"a":{"docs":{},"l":{"docs":{},"e":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.07194244604316546}},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}}}}}}},"a":{"docs":{},"v":{"docs":{},"e":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}}}}},"e":{"docs":{},"e":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}},"r":{"docs":{},"v":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}},"e":{"docs":{},"`":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}},"l":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},"t":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}},"o":{"docs":{},"u":{"docs":{},"r":{"docs":{},"c":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}},"i":{"docs":{},"c":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.019417475728155338},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"s":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}}}},"u":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"y":{"docs":{},".":{"docs":{},"m":{"docs":{},"d":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}},"i":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":5.022988505747127},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}},"c":{"docs":{},"h":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.022988505747126436},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}},"p":{"docs":{},"p":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}},"h":{"docs":{},"a":{"docs":{},"r":{"docs":{},"e":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}},"i":{"docs":{},"f":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},".":{"docs":{},"t":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}},"i":{"docs":{},"m":{"docs":{},"i":{"docs":{},"l":{"docs":{},"i":{"docs":{},"a":{"docs":{},"r":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}},"k":{"docs":{},"i":{"docs":{},"p":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}}},"y":{"docs":{},"s":{"docs":{},"t":{"docs":{},"e":{"docs":{},"m":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}},"m":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.009708737864077669}},"i":{"docs":{},"x":{"docs":{"implementation_scheme/mixed_precision_training_implementation_scheme.html":{"ref":"implementation_scheme/mixed_precision_training_implementation_scheme.html","tf":2.2},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":3.344124700239808}}}},"a":{"docs":{},"s":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.02912621359223301}},"_":{"docs":{},"b":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},"c":{"docs":{},"h":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}}}}}}}},"x":{"docs":{},"i":{"docs":{},"m":{"docs":{},"u":{"docs":{},"m":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.01079136690647482}}}}}}}},"e":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"d":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}}}},"a":{"docs":{},"n":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}},"s":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},"n":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}},"e":{"docs":{},"d":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}}},"o":{"docs":{},"v":{"docs":{},"e":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}},"d":{"docs":{},"e":{"docs":{},"l":{"docs":{"paper_reading/model_compression/":{"ref":"paper_reading/model_compression/","tf":5.5},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}},"r":{"docs":{},"e":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}},"v":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{},"i":{"docs":{},"p":{"docs":{},"l":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}},"i":{"docs":{},"c":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},"\"":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}}}}}}}}}}},"b":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.009708737864077669}},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{"tech_tools/":{"ref":"tech_tools/","tf":0.5}}}}},"a":{"docs":{},"c":{"docs":{},"k":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345},"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}},"o":{"docs":{},"f":{"docs":{},"f":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.01079136690647482}}}}}}},"t":{"docs":{},"c":{"docs":{},"h":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}},"o":{"docs":{},"a":{"docs":{},"r":{"docs":{},"t":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}},"t":{"docs":{},"h":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"h":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.019417475728155338}},",":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.014563106796116505}}},".":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.009708737864077669}}},":":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}}},"u":{"docs":{},"i":{"docs":{},"l":{"docs":{},"d":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.02912621359223301}},"`":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}},"t":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}},"y":{"docs":{},":":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}},"e":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{"tech_tools/git/":{"ref":"tech_tools/git/","tf":10},"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.05339805825242718}},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":3.367313915857605}},"'":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}},"h":{"docs":{},"u":{"docs":{},"b":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":3.3527508090614884}},",":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}},":":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}}},"e":{"docs":{},"n":{"docs":{},"e":{"docs":{},"r":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}},"h":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.02912621359223301}}},"o":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.009708737864077669}}},"r":{"docs":{},"a":{"docs":{},"d":{"docs":{},"i":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.014388489208633094}},"s":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}}}},"u":{"docs":{},"s":{"docs":{"tech_tools/git/":{"ref":"tech_tools/git/","tf":0.3333333333333333},"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.014563106796116505},"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.04597701149425287},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.02158273381294964}}},"p":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"\"":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}}}}}},"r":{"docs":{},"l":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}},"n":{"docs":{},"i":{"docs":{},"q":{"docs":{},"u":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{},"f":{"docs":{},"l":{"docs":{},"o":{"docs":{},"w":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},"l":{"docs":{},"i":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}}}}},"\"":{"docs":{},".":{"docs":{},"/":{"docs":{},"_":{"docs":{},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{},"\"":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}}}}},"g":{"docs":{},"e":{"docs":{},"n":{"docs":{},"e":{"docs":{},"r":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}},"s":{"docs":{},"e":{"docs":{},"t":{"docs":{},"t":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"\"":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}}}}},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}},"u":{"docs":{},"p":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}},"a":{"docs":{},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"a":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"\"":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}},",":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}}}}}}}},"u":{"docs":{},"t":{"docs":{},"o":{"docs":{},"m":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}}}}},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"s":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"\"":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"e":{"docs":{},"d":{"docs":{},"\"":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}}}}},"d":{"docs":{},"y":{"docs":{},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}},"i":{"docs":{},"c":{"docs":{},"\"":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}}}}}},"m":{"docs":{},"i":{"docs":{},"x":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},"#":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.024271844660194174}}},".":{"docs":{},"/":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}},"*":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.014563106796116505}}},"_":{"docs":{},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{},".":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}}},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"_":{"docs":{},"b":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},"c":{"docs":{},"h":{"docs":{},"/":{"docs":{},"_":{"docs":{},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{},"/":{"docs":{},"*":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}}}}}}}}}}}}}}}}}}}},"`":{"docs":{},"g":{"docs":{},"h":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}},"i":{"docs":{},"t":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.009708737864077669}}}}}}}}},"m":{"docs":{},"a":{"docs":{},"s":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"_":{"docs":{},"b":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{},"c":{"docs":{},"h":{"docs":{},"`":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}}}}}}}}}}}},"a":{"docs":{},"b":{"docs":{},"o":{"docs":{},"v":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}},"s":{"docs":{},"o":{"docs":{},"l":{"docs":{},"u":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}},"d":{"docs":{},"d":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.009708737864077669}}},"j":{"docs":{},"u":{"docs":{},"s":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}},"l":{"docs":{},"r":{"docs":{},"e":{"docs":{},"a":{"docs":{},"d":{"docs":{},"i":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}},"g":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"m":{"docs":{},"s":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}}}},"u":{"docs":{},"t":{"docs":{},"o":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}}}},"h":{"docs":{},"o":{"docs":{},"r":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"v":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.022988505747126436}}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"a":{"docs":{},"g":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}}},"p":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}}},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}},"c":{"docs":{},"u":{"docs":{},"l":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}},"s":{"docs":{},"e":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}},"h":{"docs":{},"e":{"docs":{},"c":{"docs":{},"k":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}},"o":{"docs":{},"u":{"docs":{},"t":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.014563106796116505}}}}}}}},"o":{"docs":{},"o":{"docs":{},"s":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},"l":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}},"`":{"docs":{},",":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}},"i":{"docs":{},"p":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}},"s":{"docs":{},",":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}},":":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}},"m":{"docs":{},"m":{"docs":{},"i":{"docs":{},"t":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.009708737864077669}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"paper_reading/model_compression/":{"ref":"paper_reading/model_compression/","tf":5.5}}}}}},"u":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},"n":{"docs":{},"g":{"docs":{},"r":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"!":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}}}},"t":{"docs":{},"r":{"docs":{},"o":{"docs":{},"l":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}},"c":{"docs":{},"r":{"docs":{},"e":{"docs":{},"t":{"docs":{},"e":{"docs":{},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.01079136690647482}}}}}}}}},"u":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}}}},"d":{"docs":{},"e":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"i":{"docs":{},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}}}}}}}},"e":{"docs":{},"p":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}},"c":{"docs":{},"r":{"docs":{},"e":{"docs":{},"a":{"docs":{},"s":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}}}}}},"i":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},"l":{"docs":{},"i":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}}},"f":{"docs":{},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}},"s":{"docs":{},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{},"b":{"docs":{},"u":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}},"o":{"docs":{},"c":{"docs":{},"s":{"docs":{},"\"":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}},"n":{"docs":{},"'":{"docs":{},"t":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}},"e":{"docs":{},",":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}}},"u":{"docs":{},"r":{"docs":{},"e":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.022988505747126436},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}},"y":{"docs":{},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.01079136690647482}}}}}}},"f":{"docs":{},"i":{"docs":{},"l":{"docs":{},"e":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.024271844660194174}},"s":{"docs":{},".":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}},"r":{"docs":{},"s":{"docs":{},"t":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.009708737864077669},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}},"x":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.022988505747126436},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}},"o":{"docs":{},"l":{"docs":{},"l":{"docs":{},"o":{"docs":{},"w":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.019417475728155338}}}}}},"c":{"docs":{},"u":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}},"\"":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}}}}}},"l":{"docs":{},"o":{"docs":{},"w":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}},"a":{"docs":{},"t":{"1":{"6":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}},"docs":{}},"docs":{}}}}},"u":{"docs":{},"n":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}}}}},"r":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{},"m":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}}}}}},"l":{"docs":{},"o":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"h":{"docs":{},"o":{"docs":{},"s":{"docs":{},"t":{"docs":{},".":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}}},"t":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.014563106796116505}}}}},"s":{"docs":{},"s":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.06115107913669065}}}},"w":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}},"e":{"docs":{},"r":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}},"g":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}},"n":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.01079136690647482}}}}}}}},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.01079136690647482}}}},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{},".":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}},")":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}},"e":{"docs":{},"e":{"docs":{},"d":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.009708737864077669}},",":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}},"t":{"docs":{},"w":{"docs":{},"o":{"docs":{},"r":{"docs":{},"k":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.022988505747126436},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}},"u":{"docs":{},"r":{"docs":{},"a":{"docs":{},"l":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.022988505747126436},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},"x":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}}},"o":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}},"w":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.009708737864077669},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}},"r":{"docs":{},"m":{"docs":{},"a":{"docs":{},"l":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}},"u":{"docs":{},"m":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.009708737864077669}}}}}}},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.034482758620689655}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}},"s":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}}}}}}},"n":{"docs":{},"s":{"docs":{},"e":{"docs":{},"q":{"2":{"docs":{},"s":{"docs":{},"e":{"docs":{},"q":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.01079136690647482}}}}}},"docs":{}}}}}},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.01079136690647482}}}}}},"n":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}},"e":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"f":{"docs":{},"l":{"docs":{},"o":{"docs":{},"w":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.01079136690647482}},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.014388489208633094}}}}}}}}}}},"r":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}},"e":{"docs":{},"a":{"docs":{},"d":{"docs":{"paper_reading/":{"ref":"paper_reading/","tf":5.5},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}},"m":{"docs":{},"e":{"docs":{},".":{"docs":{},"m":{"docs":{},"d":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}}}},"l":{"docs":{},"l":{"docs":{},"i":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},"p":{"docs":{},"l":{"docs":{},"a":{"docs":{},"c":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}}}},"s":{"docs":{},"p":{"docs":{},"o":{"docs":{},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}},"y":{"docs":{},"\"":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}},"'":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.009708737864077669}}}}}}}}}},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"v":{"docs":{},"e":{"docs":{},"l":{"docs":{},"y":{"docs":{},")":{"docs":{},".":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}}}}}},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.022988505747126436}},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},":":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}},"c":{"docs":{},"i":{"docs":{},"p":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},"f":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}},"m":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.009708737864077669}}},"o":{"docs":{},"o":{"docs":{},"t":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}},"u":{"docs":{},"n":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.014563106796116505},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}},"l":{"docs":{},"e":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}},"a":{"docs":{},"n":{"docs":{},"g":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}},"r":{"docs":{},"e":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}},"v":{"docs":{},"i":{"docs":{},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}},"a":{"docs":{},"l":{"docs":{},"u":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.007194244604316547}}}},"r":{"docs":{},"i":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}},"y":{"docs":{},"o":{"docs":{},"u":{"docs":{},".":{"docs":{"tech_tools/git/gitbook_on_github-page.html":{"ref":"tech_tools/git/gitbook_on_github-page.html","tf":0.0048543689320388345}}}}}},"h":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}},"p":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}},"s":{"docs":{},",":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.022988505747126436}}}}}}},"e":{"docs":{},"r":{"docs":{},"e":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}},"t":{"docs":{},"t":{"docs":{},"p":{"docs":{},"s":{"docs":{},":":{"docs":{},"/":{"docs":{},"/":{"docs":{},"a":{"docs":{},"r":{"docs":{},"x":{"docs":{},"i":{"docs":{},"v":{"docs":{},".":{"docs":{},"o":{"docs":{},"r":{"docs":{},"g":{"docs":{},"/":{"docs":{},"a":{"docs":{},"b":{"docs":{},"s":{"docs":{},"/":{"1":{"7":{"1":{"0":{"docs":{},".":{"0":{"3":{"7":{"4":{"0":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}},"docs":{}}}}}}}}}}}}}}},"d":{"docs":{},"e":{"docs":{},"v":{"docs":{},"b":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{},"s":{"docs":{},".":{"docs":{},"n":{"docs":{},"v":{"docs":{},"i":{"docs":{},"d":{"docs":{},"i":{"docs":{},"a":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"/":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"x":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}}}}}}}}}}}}}}}}}},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"u":{"docs":{},"b":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"/":{"docs":{},"n":{"docs":{},"v":{"docs":{},"i":{"docs":{},"d":{"docs":{},"i":{"docs":{},"a":{"docs":{},"/":{"docs":{},"a":{"docs":{},"p":{"docs":{},"e":{"docs":{},"x":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"v":{"docs":{},"i":{"docs":{},"d":{"docs":{},"i":{"docs":{},"a":{"docs":{},".":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"u":{"docs":{},"b":{"docs":{},".":{"docs":{},"i":{"docs":{},"o":{"docs":{},"/":{"docs":{},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"s":{"docs":{},"e":{"docs":{},"q":{"2":{"docs":{},"s":{"docs":{},"e":{"docs":{},"q":{"docs":{},"/":{"docs":{},"h":{"docs":{},"t":{"docs":{},"m":{"docs":{},"l":{"docs":{},"/":{"docs":{},"m":{"docs":{},"i":{"docs":{},"x":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"k":{"docs":{},"e":{"docs":{},"y":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}},"e":{"docs":{},"p":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":0.011494252873563218}}}}},"n":{"docs":{},"o":{"docs":{},"w":{"docs":{},"l":{"docs":{},"e":{"docs":{},"d":{"docs":{},"g":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}}}},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"n":{"docs":{},"t":{"docs":{},"i":{"docs":{},"z":{"docs":{"paper_reading/model_compression/quantization_summary.html":{"ref":"paper_reading/model_compression/quantization_summary.html","tf":5.022988505747127},"paper_reading/model_compression/quantization/":{"ref":"paper_reading/model_compression/quantization/","tf":11}}}}}}}}},"(":{"docs":{},"d":{"docs":{},"e":{"docs":{},"f":{"docs":{},"a":{"docs":{},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.01079136690647482}}}}}}}}},"h":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{"paper_reading/model_compression/quantization/mixed_precision_training.html":{"ref":"paper_reading/model_compression/quantization/mixed_precision_training.html","tf":0.0035971223021582736}}}}}}}},"length":376},"corpusTokens":["\"./_book\"","\"autom","\"averaging\"","\"averaging\",","\"compensate\".","\"compressed\"","\"dynam","\"dynamic\"","\"gener","\"mix","\"setting\"","\"the","\"train","\"updat","#","(default","(henc","./","./*","./_book.","./master_barnch/_book/*","0.001).","1.","1.1.","1.1.1.","1.2.","1.2.1.","1.2.2.","2)","2,","2.","2.1.","2.1.1.","2.1.2.","2000","3.","4.","`gh","`git","`gitbook","`master_barnch`","abov","absolut","activ","add","adjust","algorithms,","alreadi","approach,","author","automat","averag","b","back","backoff","batch","begin","blog","boart","both","branch","branch,","branch.","branch:","build","build`","built","by:","calcul","call","case","check","checkout","choos","clip","clone","clone`,","code","codes,","codes:","commit","compress","comput","concretely,","congrates!","constant","control","current","data","decreas","deep","dependencies,","differ","directli","distribut","docs\"","don't","done,","dure","dynam","e.g.,","each","easi","effect","end","ensur","estim","event","everyth","execut","experience.","exponent.","extens","factor","factor\"","factor\".","factor,","factor.","file","files.","first","fix","float16","float16,","flow","focu","follow","function,","furthermore,","gener","gh","git","gitbook","gitbook'","github","github,","github:","go","gradient","gradients.","hand,","happen","happens,","here","higher","https://arxiv.org/abs/1710.03740","https://devblogs.nvidia.com/apex","https://github.com/nvidia/apex","https://nvidia.github.io/openseq2seq/html/mix","idea","implement","implementation.","includ","increas","industri","init","inspect","instal","instance,","instruction,","inter","introduc","introduct","is,","iter","iteration.","iwiki","iwiki.","keep","key","kind","knowledg","larg","last,","less","localhost.","locat","log","lognorm","loss","low","lower","m","master","master_barnch","maximum","mean","means,","mention","mentioned,","method","mix","model","more","move","multipl","multiplications\".","mv","name),","name.","need","need,","network","neural","next","normal","notic","now","number","on","one.","openseq2seq","oper","operation,","operations.","optim","origin","overflow","overflow,","pact","page","pages.","pages`","paper","paramet","parameter","path","path,","period","person","point\"","point\",","possibl","precis","precision,","precision.html#","presenc","probabl","propogation,","push","put","pytorch","quantiz","r","rang","rare","read","readme.md","realli","recip","refer","reference:","replac","repres","respectively).","respositori","respository\"","respository'","rf","rm","root","rule","run","s.t.","samples,","save","scale","scale.","scaling.","scheme","see","select","serv","serve`","set","share","shift","similiar","skip","sourc","state","static","statistics,","such","summari","summary.md","support","system","tech","them.","then,","time","time,","tip","togeth","tool","tradit","train","training\"","training.","training/","two","type,","underflow","underli","understand","understand,","uniqu","update.","updates\"","url","us","valu","varianc","visit","way,","websit","whenev","wich","wiki","you."],"pipeline":["stopWordFilter","stemmer"]},"store":{"./":{"url":"./","title":"Introduction","keywords":"","body":"iwiki\nThis is my personal wiki wich include some of tips or experience.\n"},"implementation_scheme/":{"url":"implementation_scheme/","title":"Implementation Scheme","keywords":"","body":"Implementation Scheme\n"},"implementation_scheme/mixed_precision_training_implementation_scheme.html":{"url":"implementation_scheme/mixed_precision_training_implementation_scheme.html","title":"mixed-precision training implementation scheme","keywords":"","body":"mixed-precision training implementation scheme\n"},"tech_tools/":{"url":"tech_tools/","title":"Tech Tools","keywords":"","body":"Tech Blog\n"},"tech_tools/git/":{"url":"tech_tools/git/","title":"Git","keywords":"","body":"tools using tips\n"},"tech_tools/git/gitbook_on_github-page.html":{"url":"tech_tools/git/gitbook_on_github-page.html","title":"Gitbook On Github-page","keywords":"","body":"gitbook on github-page\n1. Building a Respository on Github, e.g., iwiki.\n2. Building a Branch Called gh-pages\nAfter building following branch, github will automatically serves a github-page for you.\ngit checkout -b gh-pages\n\n3. Building Gitbook Static Website Files\nFirst go back to master branch:\ngit checkout master\n\nThen, after installing gitbook and its dependencies, build README.md and SUMMARY.md following gitbook \ninstruction, and run following codes to generate static website files which were located at ./_book.\ngitbook init\ngitbook build\n# Can also use `gitbook serve` to replace `gitbook build` which can let\n# you see your website on localhost.\n\nNow we can push all these files to master branch, by the way, the master branch is used for saving your \ngitbook's source files.\ngit add ./*\ngit commit -m \"General updates\"\ngit push origin master\n\n4. Moving Gitbook Static Website Files To gh-pages Branch\nFirst go to gh-pages branch by:\ngit checkout gh-pages\n\nNotice this time we don't need -b since we have already built this branch.  \nAfter running above codes, we need all the static website files in \"./_book\" located \nat master branch, so the most directly method is running following codes:\n# The current state is, located at the respository's root path, with `gh-pages`\n# branch. After `git clone`, use the `master_barnch` represents the \n# respository's path name.\n\ngit rm -r ./*\ngit clone \"the respository\"\nmv ./master_barnch/_book/* ./\nrm -rf master_barnch\n\nNow we get all we need, so push to github:\ngit add ./*\ngit commit -m \"Update docs\"\ngit push original gh-pages\n\nCongrates! Everything done, you can visit the \"Setting\" boart to get the url \nof your github pages.\n"},"paper_reading/":{"url":"paper_reading/","title":"Paper Reading","keywords":"","body":"Paper Reading\n"},"paper_reading/model_compression/":{"url":"paper_reading/model_compression/","title":"Model Compression","keywords":"","body":"Model Compression\n"},"paper_reading/model_compression/quantization_summary.html":{"url":"paper_reading/model_compression/quantization_summary.html","title":"Quantization Summary","keywords":"","body":"Quantization Summary\n1. Paper Summary\n1.1. PACT- Parameterized Clipping Activation for Quantized Neural Networks\n1.1.1. When Using Lower Precision, When Using Higher\nDuring some operations which include \"averaging\", such as the calculation of gradients \nduring back propogation, since averaging has effect of \"compensate\". On the other hand, \nsome operation, such as traditional activation function, which do not include \"averaging\" \noperation across batch samples, so we should use higher precision to execute this kind \nof operations.\n1.2. Training deep neural networks with low precision multiplications\n1.2.1. Key\nUse the idea of \"dynamic fixed point\", the \"dynamic\" means the precision of \nthe data flow to different operations has unique shared fixed exponent. Refer \nto page 3.  \n1.2.2. How To Dynamically Control Loss Scale Factor\nRefer to page 3.\n"},"paper_reading/model_compression/quantization/":{"url":"paper_reading/model_compression/quantization/","title":"Quantization","keywords":"","body":"Quantization\n"},"paper_reading/model_compression/quantization/mixed_precision_training.html":{"url":"paper_reading/model_compression/quantization/mixed_precision_training.html","title":"Mixed Precision Training","keywords":"","body":"Mixed Precision Training\n\nreference:  https://arxiv.org/abs/1710.03740  https://github.com/NVIDIA/apex  https://nvidia.github.io/OpenSeq2Seq/html/mixed-precision.html#  https://devblogs.nvidia.com/apex-pytorch-easy-mixed-precision-training/ \n\n1. Introduction\nThe paper \"Mixed Precision Training\" is easy to read after understanding \nsome underlying computer system knowledge such as data type, so here just \nintroduce some tips and extension of the paper by summary some industrial \nimplementation.  \n2. Extension\n2.1. Loss Scaling\nAs the paper mentioned, loss scaling is really useful when putting the \"compressed\" \nparameters into the range in which lower data precision can represent them. This tech \ncould be used in two approach, one is using static loss scaling factor, other is using \ndynamic loss scaling factor. The first one is easy to understand, now focus on the \ndynamic one.  \nThe paper mentioned \"automating loss-scaling factor\" at last, the author means, for \ninstance, if some gradients or parameters happened overflow, which means these number \nis to large to represented by float16, so we should decrease loss-scaling factor, on \nthe other hand, if underflow happens, in this case we should increase loss-scaling \nfactor.  \nOpenSeq2Seq did more on \"automating loss-scaling factor\". OpenSeq2Seq implements \nan extension to the mixed precision recipe that we call automatic loss scaling. \nThe optimizer inspects the parameter gradients at each iteration and uses their \nvalues to select the loss scale for the next iteration.  Concretely, OpenSeq2Seq \nhas support for two automatic loss scaling algorithms, Backoff and LogNormal scaling.  \n2.1.1. Backoff Scaling\nBackoff scaling begins with a large loss scale and checks for overflow in the \nparameter gradients at the end of each iteration. Whenever there is an overflow, \nthe loss scale decreases by a constant factor (default is 2) and the optimizer will \nskip the update. Furthermore, if there has been no overflow for a period of time, \nthe loss scale increases by a constant factor (defaults are 2000 iterations and 2, \nrespectively). These two rules together ensure both that the loss scale is as large \nas possible and also that it can adjust to shifting dynamic range during training.  \nThis scaling method is similiar with \"dynamic fixed point\" method mentioned in \n\"Training deep neural networks with low precision multiplications\".\n2.1.2. LogNormal Scaling\nLogNormal scaling uses gradient statistics, rather than the presence of overflow, \nto set the loss scale. It keeps a running estimate of the mean and variance of \nthe inter-iteration maximum absolute value of the parameter gradients. It models \nthe inter-iteration maximum as log-normally distributed (hence the name), and then \nchooses the loss scale for the next iteration s.t. the probability of the maximum \noverflowing float16 is less than some constant (default is 0.001). In the rare \nevent of an overflow, the optimizer skips the update.\n"}}}